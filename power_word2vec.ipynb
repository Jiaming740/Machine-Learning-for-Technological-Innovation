{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Power Word2Vec** The code is to train power Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d8715679305b1f7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\djm\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.456 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved to Embeddings/Power_data_word2vec.model\n",
      "Word vectors saved to Embeddings/word2vec_vectors.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "# Step 1: Load the TSV file\n",
    "file_path = 'Training Data/power_data.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Step 2: Extract the 'abstract' column\n",
    "abstracts = df['abstract'].tolist()\n",
    "\n",
    "# Step 3: Tokenize the abstracts using jieba for Chinese text\n",
    "def tokenize_text(text):\n",
    "    return list(jieba.lcut(text))\n",
    "\n",
    "# Tokenize all abstracts\n",
    "tokenized_abstracts = [tokenize_text(abstract) for abstract in abstracts]\n",
    "\n",
    "# Step 4: Train Word2Vec model\n",
    "# Define model parameters\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_abstracts,   # Input tokenized sentences\n",
    "    vector_size=300,                 # Size of word vectors\n",
    "    window=5,                        # Context window size\n",
    "    min_count=2,                     # Minimum word frequency\n",
    "    workers=4,                       # Number of CPU threads for parallelism\n",
    "    sg=0                             # Training algorithm: 0 for CBOW, 1 for skip-gram\n",
    ")\n",
    "\n",
    "# Step 5: Save the trained Word2Vec model\n",
    "model_path = 'Embeddings/Power_data_word2vec.model'\n",
    "word2vec_model.save(model_path)\n",
    "\n",
    "# Optionally, save the word vectors in text format\n",
    "word_vectors_path = 'Embeddings/word2vec_vectors.txt'\n",
    "word2vec_model.wv.save_word2vec_format(word_vectors_path, binary=False)\n",
    "\n",
    "print(f\"Word2Vec model saved to {model_path}\")\n",
    "print(f\"Word vectors saved to {word_vectors_path}\")\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T07:39:35.693603200Z",
     "start_time": "2024-09-10T07:39:23.216123400Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1cdd673c70b6a737"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
